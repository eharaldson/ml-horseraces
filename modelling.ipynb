{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ProcessedDataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_to_graph_data(df: pd.DataFrame, \n",
    "                                 label: str, \n",
    "                                 date_time_col: str = 'DateTime', \n",
    "                                 track_name_col: str = 'Track Name'):\n",
    "    \n",
    "    # Group the rows by DateTime and Track Name\n",
    "    grouped = df.groupby([date_time_col, track_name_col])\n",
    "\n",
    "    graph_data = []\n",
    "    \n",
    "    # Iterate over each group\n",
    "    for (dt, track), group_df in grouped:\n",
    "        \n",
    "        # Drop unused columns and convert to numeric\n",
    "        X = group_df.drop(columns=[label, date_time_col, track_name_col]).astype(float).to_numpy()\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "        # Convert the label column to torch tensor\n",
    "        y = torch.tensor(group_df[label].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        # Append (X, y) to the list\n",
    "        graph_data.append([X, y])\n",
    "\n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = transform_data_to_graph_data(df, 'win')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCNSelfLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Graph Convolution layer:\n",
    "      H_next = A_hat * H * W\n",
    "    where A_hat is the adjacency matrix (possibly normalized),\n",
    "    H is the input node features, and W is a learnable weight.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Learnable weight matrix: shape (in_features, out_features)\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "\n",
    "        # Optional bias: shape (out_features)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # A simple initialization scheme (e.g., glorot/xavier)\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, H):\n",
    "        \"\"\"\n",
    "        H: [N, in_features]   - node feature matrix\n",
    "        A_hat: [N, N]         - adjacency matrix (ideally normalized)\n",
    "        returns: [N, out_features]\n",
    "        \"\"\"\n",
    "        # 1) Fully connected graph - normalised by the degree to avoid vanishing/exploding gradients\n",
    "        A_hat = torch.eye(H.shape[0])\n",
    "\n",
    "        # 2) Multiply input features by W\n",
    "        HW = torch.matmul(H, self.weight)  # [N, out_features]\n",
    "\n",
    "        # 3) Propagate/aggregate over adjacency\n",
    "        #    A_hat * (H * W)\n",
    "        out = torch.matmul(A_hat, HW)       # [N, out_features]\n",
    "\n",
    "        # 3) Add bias (if any)\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out\n",
    "    \n",
    "class GCNNonSelfLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Graph Convolution layer:\n",
    "      H_next = A_hat * H * W\n",
    "    where A_hat is the adjacency matrix (possibly normalized),\n",
    "    H is the input node features, and W is a learnable weight.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Learnable weight matrix: shape (in_features, out_features)\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "\n",
    "        # Optional bias: shape (out_features)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # A simple initialization scheme (e.g., glorot/xavier)\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, H):\n",
    "        \"\"\"\n",
    "        H: [N, in_features]   - node feature matrix\n",
    "        A_hat: [N, N]         - adjacency matrix (ideally normalized)\n",
    "        returns: [N, out_features]\n",
    "        \"\"\"\n",
    "        # 1) Fully connected graph - normalised by the degree to avoid vanishing/exploding gradients\n",
    "        A_hat = torch.ones((H.shape[0], H.shape[0])) - torch.eye(H.shape[0])\n",
    "        A_hat = A_hat / H.shape[0]\n",
    "\n",
    "        # 2) Multiply input features by W\n",
    "        HW = torch.matmul(H, self.weight)  # [N, out_features]\n",
    "\n",
    "        # 3) Propagate/aggregate over adjacency\n",
    "        #    A_hat * (H * W)\n",
    "        out = torch.matmul(A_hat, HW)       # [N, out_features]\n",
    "\n",
    "        # 3) Add bias (if any)\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out\n",
    "    \n",
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Graph Convolution layer:\n",
    "      H_next = A_hat * H * W\n",
    "    where A_hat is the adjacency matrix (possibly normalized),\n",
    "    H is the input node features, and W is a learnable weight.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Learnable weight matrix: shape (in_features, out_features)\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "\n",
    "        # Optional bias: shape (out_features)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # A simple initialization scheme (e.g., glorot/xavier)\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, H):\n",
    "        \"\"\"\n",
    "        H: [N, in_features]   - node feature matrix\n",
    "        A_hat: [N, N]         - adjacency matrix (ideally normalized)\n",
    "        returns: [N, out_features]\n",
    "        \"\"\"\n",
    "        # 1) Fully connected graph - normalised by the degree to avoid vanishing/exploding gradients\n",
    "        A_hat = torch.ones((H.shape[0], H.shape[0])) / H.shape[0] + torch.eye(H.shape[0])\n",
    "\n",
    "        # 2) Multiply input features by W\n",
    "        HW = torch.matmul(H, self.weight)  # [N, out_features]\n",
    "\n",
    "        # 3) Propagate/aggregate over adjacency\n",
    "        #    A_hat * (H * W)\n",
    "        out = torch.matmul(A_hat, HW)       # [N, out_features]\n",
    "\n",
    "        # 3) Add bias (if any)\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGCN1(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.gc1 = GCNLayer(in_features, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, H):\n",
    "        x = self.gc1(H)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleGCN2(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim1, hidden_dim2):\n",
    "        super().__init__()\n",
    "        self.gc1 = GCNLayer(in_features, hidden_dim1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gc2 = GCNLayer(hidden_dim1, hidden_dim2)\n",
    "        self.linear = nn.Linear(hidden_dim2, 1)\n",
    "\n",
    "    def forward(self, H):\n",
    "        x = self.relu(self.gc1(H))\n",
    "        x = self.gc2(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Training Loss: 8.4561, Validation Loss: 6.1249\n",
      "Epoch [20/500], Training Loss: 6.8188, Validation Loss: 4.4333\n",
      "Epoch [30/500], Training Loss: 6.0622, Validation Loss: 2.3490\n",
      "Epoch [40/500], Training Loss: 5.1724, Validation Loss: 1.6676\n",
      "Epoch [50/500], Training Loss: 5.4594, Validation Loss: 8.2519\n",
      "Epoch [60/500], Training Loss: 4.5003, Validation Loss: 4.2570\n",
      "Epoch [70/500], Training Loss: 4.0544, Validation Loss: 0.9388\n",
      "Epoch [80/500], Training Loss: 3.4256, Validation Loss: 2.0264\n",
      "Epoch [90/500], Training Loss: 3.4219, Validation Loss: 7.4251\n",
      "Epoch [100/500], Training Loss: 2.7956, Validation Loss: 5.7092\n",
      "Epoch [110/500], Training Loss: 2.3113, Validation Loss: 5.5580\n",
      "Epoch [120/500], Training Loss: 1.9300, Validation Loss: 1.3019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[388], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(validation_graphs)):\n\u001b[0;32m---> 45\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(validation_graphs[i][\u001b[38;5;241m0\u001b[39m])   \n\u001b[1;32m     46\u001b[0m     true_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(validation_graphs[i][\u001b[38;5;241m1\u001b[39m], (\u001b[38;5;28mlen\u001b[39m(validation_graphs[i][\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predictions, true_labels)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[380], line 8\u001b[0m, in \u001b[0;36mSimpleGCN1.forward\u001b[0;34m(self, H)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, H):\n\u001b[0;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgc1(H)\n\u001b[1;32m      9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[250], line 150\u001b[0m, in \u001b[0;36mGCNLayer.forward\u001b[0;34m(self, H)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03mH: [N, in_features]   - node feature matrix\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03mA_hat: [N, N]         - adjacency matrix (ideally normalized)\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mreturns: [N, out_features]\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# 1) Fully connected graph - normalised by the degree to avoid vanishing/exploding gradients\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m A_hat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    151\u001b[0m A_hat \u001b[38;5;241m=\u001b[39m A_hat \u001b[38;5;241m/\u001b[39m H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# 2) Multiply input features by W\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "val_fraction = 0.2\n",
    "data_split_indx = round(len(graphs) * (1-val_fraction))\n",
    "training_graphs, validation_graphs = graphs[:data_split_indx], graphs[data_split_indx:]\n",
    "\n",
    "model = SimpleGCN1(graphs[0][0].shape[1], 64)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 4. Training loop\n",
    "num_epochs = 500\n",
    "num_per_epoch = 10000\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    training_epoch_loss = 0\n",
    "    for i in range(num_per_epoch):\n",
    "        indx = random.randint(0, len(training_graphs)-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(training_graphs[indx][0])   \n",
    "        true_labels = torch.reshape(training_graphs[indx][1], (-1, 1))\n",
    "        loss = criterion(predictions, true_labels)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_epoch_loss += loss.item()\n",
    "\n",
    "    # Print loss periodically\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "\n",
    "        # Calculate Validation loss\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        for i in range(len(validation_graphs)):\n",
    "\n",
    "            predictions = model(validation_graphs[i][0])   \n",
    "            true_labels = torch.reshape(validation_graphs[i][1], (len(validation_graphs[i][1]), 1))\n",
    "            loss = criterion(predictions, true_labels)\n",
    "\n",
    "            validation_loss += loss\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {training_epoch_loss/num_per_epoch:.4f}, Validation Loss: {validation_loss/len(validation_graphs):.4f}\")\n",
    "\n",
    "    \n",
    "\n",
    "# # 5. Evaluate the model on the entire dataset\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     preds = model(x)\n",
    "#     final_loss = criterion(preds, y)\n",
    "    \n",
    "# print(\"\\nTraining complete!\")\n",
    "# print(f\"Final Loss over all data: {final_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_features, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, H):\n",
    "        x = self.lin1(H)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(df.drop(columns=['win', 'DateTime', 'Track Name']).astype(np.float32).to_numpy())\n",
    "y = torch.tensor(df['win'].astype(np.float32).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 32\n",
    "validation_fraction = 0.1\n",
    "validation_index = round(len(y)*(1-validation_fraction))\n",
    "\n",
    "X_train = X[:validation_index,:]\n",
    "X_validation = X[validation_index:,:]\n",
    "y_train = y[:validation_index]\n",
    "y_validation = y[validation_index:]\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataset = TensorDataset(X_validation, y_validation)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=len(validation_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 2.3043, Validation loss: 2.1002\n",
      "Epoch 2/500, Loss: 0.7374, Validation loss: 1.4705\n",
      "Epoch 3/500, Loss: 0.7067, Validation loss: 0.6810\n",
      "Epoch 4/500, Loss: 0.6733, Validation loss: 0.4288\n",
      "Epoch 5/500, Loss: 0.6742, Validation loss: 0.5574\n",
      "Epoch 6/500, Loss: 0.6496, Validation loss: 0.4397\n",
      "Epoch 7/500, Loss: 0.6447, Validation loss: 0.5008\n",
      "Epoch 8/500, Loss: 0.6148, Validation loss: 0.8251\n",
      "Epoch 9/500, Loss: 0.6057, Validation loss: 0.8874\n",
      "Epoch 10/500, Loss: 0.5993, Validation loss: 0.3493\n",
      "Epoch 11/500, Loss: 0.5861, Validation loss: 0.3840\n",
      "Epoch 12/500, Loss: 0.5859, Validation loss: 0.7026\n",
      "Epoch 13/500, Loss: 0.5673, Validation loss: 0.6976\n",
      "Epoch 14/500, Loss: 0.5489, Validation loss: 0.6212\n",
      "Epoch 15/500, Loss: 0.5321, Validation loss: 0.4282\n",
      "Epoch 16/500, Loss: 0.5281, Validation loss: 0.4145\n",
      "Epoch 17/500, Loss: 0.5276, Validation loss: 0.5642\n",
      "Epoch 18/500, Loss: 0.5062, Validation loss: 0.5810\n",
      "Epoch 19/500, Loss: 0.4925, Validation loss: 0.3526\n",
      "Epoch 20/500, Loss: 0.4960, Validation loss: 1.0907\n",
      "Epoch 21/500, Loss: 0.4949, Validation loss: 0.3744\n",
      "Epoch 22/500, Loss: 0.4843, Validation loss: 0.3659\n",
      "Epoch 23/500, Loss: 0.4730, Validation loss: 0.6020\n",
      "Epoch 24/500, Loss: 0.4720, Validation loss: 0.3229\n",
      "Epoch 25/500, Loss: 0.4561, Validation loss: 0.3359\n",
      "Epoch 26/500, Loss: 0.4512, Validation loss: 0.3437\n",
      "Epoch 27/500, Loss: 0.4450, Validation loss: 0.3819\n",
      "Epoch 28/500, Loss: 0.4269, Validation loss: 0.3257\n",
      "Epoch 29/500, Loss: 0.4185, Validation loss: 0.4321\n",
      "Epoch 30/500, Loss: 0.4175, Validation loss: 0.9063\n",
      "Epoch 31/500, Loss: 0.4117, Validation loss: 1.0956\n",
      "Epoch 32/500, Loss: 0.3926, Validation loss: 0.3305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m num_in_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# 1. Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# 2. Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "model = Classification(X.shape[1], 32)\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_in_epoch = 0\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # 1. Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # 3. Compute the loss (BCEWithLogitsLoss expects raw logits from the final layer)\n",
    "        loss = criterion(outputs, torch.reshape(batch_y, (-1,1)))\n",
    "        \n",
    "        # 4. Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_in_epoch += 1\n",
    "    \n",
    "    # Evaluation \n",
    "    model.eval()\n",
    "    for batch_x, batch_y in validation_loader:\n",
    "        # 2. Forward pass\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        validation_loss = criterion(outputs, torch.reshape(batch_y, (-1,1)))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/num_in_epoch:.4f}, Validation loss: {validation_loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_x, batch_y in validation_loader:\n",
    "    outputs = model(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sigmoid.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSigmoid(outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:485\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.__init__() got an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m     )\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(args):\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.__init__() takes 1 positional argument but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m were\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m given\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m     )\n\u001b[1;32m    490\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;124;03msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Sigmoid.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "torch.nn.Sigmoid(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model  (int): Dimensionality of the model (embeddings).\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(CrossAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads  # dimensionality per head\n",
    "        \n",
    "        # Learnable linear projections for Q, K, V\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Final linear layer to recombine all heads\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, context, mask=None):\n",
    "        \"\"\"\n",
    "        Computes cross attention between x (query) and context (key/value).\n",
    "        \n",
    "        Args:\n",
    "            x       (torch.Tensor): [batch_size, seq_len, d_model] – Queries\n",
    "            context (torch.Tensor): [batch_size, context_len, d_model] – Keys/Values\n",
    "            mask    (torch.Tensor): [batch_size, 1, 1, context_len] – Optional attention mask (0 where masked)\n",
    "        \n",
    "        Returns:\n",
    "            out (torch.Tensor): The attended representation of x, shape [batch_size, seq_len, d_model].\n",
    "            attention_weights (torch.Tensor): Attention weights of shape [batch_size, num_heads, seq_len, context_len].\n",
    "        \"\"\"\n",
    "        B, Tx, _ = x.shape\n",
    "        Bc, Tc, _ = context.shape\n",
    "        assert B == Bc, \"Query and context must have the same batch size.\"\n",
    "\n",
    "        # 1. Linear Projections\n",
    "        Q = self.Wq(x)       # [B, Tx, d_model]\n",
    "        K = self.Wk(context) # [B, Tc, d_model]\n",
    "        V = self.Wv(context) # [B, Tc, d_model]\n",
    "\n",
    "        # 2. Reshape and transpose for multi-head attention\n",
    "        #    from [B, Tx, d_model] -> [B, num_heads, Tx, depth]\n",
    "        Q = Q.view(B, Tx, self.num_heads, self.depth).transpose(1, 2)  # [B, num_heads, Tx, depth]\n",
    "        K = K.view(B, Tc, self.num_heads, self.depth).transpose(1, 2)  # [B, num_heads, Tc, depth]\n",
    "        V = V.view(B, Tc, self.num_heads, self.depth).transpose(1, 2)  # [B, num_heads, Tc, depth]\n",
    "\n",
    "        # 3. Scaled Dot-Product Attention\n",
    "        #    attention_scores: [B, num_heads, Tx, Tc]\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.depth)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # mask == 0 where we want to mask\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)  # [B, num_heads, Tx, Tc]\n",
    "        \n",
    "        # 4. Combine attention weights with values: [B, num_heads, Tx, depth]\n",
    "        out = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 5. Reshape back: [B, Tx, num_heads, depth] -> [B, Tx, d_model]\n",
    "        out = out.transpose(1, 2).contiguous().view(B, Tx, self.d_model)\n",
    "        \n",
    "        # 6. Apply final projection\n",
    "        out = self.fc_out(out)\n",
    "        \n",
    "        return out, attention_weights\n",
    "\n",
    "\n",
    "class SetTransformer(nn.Module):\n",
    "    def __init__(self, dim_input, num_outputs, dim_output,\n",
    "            num_inds=32, dim_hidden=128, num_heads=4, ln=False):\n",
    "        super(SetTransformer, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dec(self.enc(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_X = graphs[0][0]\n",
    "graph_y = graphs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pma = PMA(graph_X.shape[1], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1238, -0.1221,  0.0420, -0.0408,  0.0734,  0.0364,  0.0165,\n",
      "           0.0541,  0.0391, -0.0768, -0.0652, -0.1013, -0.0599, -0.0190,\n",
      "           0.0491, -0.1362, -0.0462, -0.0349,  0.0358, -0.1169, -0.0418,\n",
      "           0.1106,  0.0367,  0.0574,  0.1245, -0.0562,  0.0933,  0.0673,\n",
      "          -0.0715, -0.0657,  0.0906,  0.0146, -0.0973, -0.0226,  0.0438,\n",
      "          -0.1416, -0.1278, -0.1001,  0.0519,  0.1368, -0.0944, -0.1301,\n",
      "           0.1151, -0.0559,  0.0450, -0.1343, -0.0219, -0.0382,  0.1387,\n",
      "           0.0247,  0.0907, -0.0661, -0.1329,  0.0242,  0.1082,  0.1007,\n",
      "          -0.0115,  0.0537,  0.0439,  0.1334, -0.0432,  0.0061,  0.1171,\n",
      "          -0.0680,  0.1277,  0.1181, -0.0460,  0.0634, -0.0178,  0.0561,\n",
      "          -0.1169,  0.1064, -0.0148, -0.0384, -0.0318,  0.1126, -0.0193,\n",
      "           0.0851, -0.1072,  0.0472, -0.0466, -0.0720, -0.0914,  0.0522,\n",
      "          -0.0345,  0.0587,  0.0566,  0.1409,  0.1207, -0.1106, -0.0406,\n",
      "          -0.0816, -0.0962, -0.1223, -0.0644,  0.1297, -0.1083,  0.0930,\n",
      "           0.1150, -0.0735,  0.0028,  0.0964, -0.1102,  0.0187,  0.1286,\n",
      "           0.0921, -0.1113,  0.0865, -0.0367, -0.0893, -0.0765, -0.1379,\n",
      "          -0.0951, -0.0028,  0.0997, -0.0506, -0.0107, -0.1355,  0.0400,\n",
      "           0.1333, -0.0210,  0.0189,  0.0169,  0.0744,  0.0454, -0.0676,\n",
      "          -0.1210, -0.0486,  0.0269,  0.0659, -0.1328, -0.0636, -0.1140,\n",
      "           0.0248, -0.0072,  0.0932, -0.0590, -0.0233, -0.1226,  0.0883,\n",
      "          -0.0500,  0.0264, -0.0554,  0.0326,  0.0934]],\n",
      "\n",
      "        [[ 0.1238, -0.1221,  0.0420, -0.0408,  0.0734,  0.0364,  0.0165,\n",
      "           0.0541,  0.0391, -0.0768, -0.0652, -0.1013, -0.0599, -0.0190,\n",
      "           0.0491, -0.1362, -0.0462, -0.0349,  0.0358, -0.1169, -0.0418,\n",
      "           0.1106,  0.0367,  0.0574,  0.1245, -0.0562,  0.0933,  0.0673,\n",
      "          -0.0715, -0.0657,  0.0906,  0.0146, -0.0973, -0.0226,  0.0438,\n",
      "          -0.1416, -0.1278, -0.1001,  0.0519,  0.1368, -0.0944, -0.1301,\n",
      "           0.1151, -0.0559,  0.0450, -0.1343, -0.0219, -0.0382,  0.1387,\n",
      "           0.0247,  0.0907, -0.0661, -0.1329,  0.0242,  0.1082,  0.1007,\n",
      "          -0.0115,  0.0537,  0.0439,  0.1334, -0.0432,  0.0061,  0.1171,\n",
      "          -0.0680,  0.1277,  0.1181, -0.0460,  0.0634, -0.0178,  0.0561,\n",
      "          -0.1169,  0.1064, -0.0148, -0.0384, -0.0318,  0.1126, -0.0193,\n",
      "           0.0851, -0.1072,  0.0472, -0.0466, -0.0720, -0.0914,  0.0522,\n",
      "          -0.0345,  0.0587,  0.0566,  0.1409,  0.1207, -0.1106, -0.0406,\n",
      "          -0.0816, -0.0962, -0.1223, -0.0644,  0.1297, -0.1083,  0.0930,\n",
      "           0.1150, -0.0735,  0.0028,  0.0964, -0.1102,  0.0187,  0.1286,\n",
      "           0.0921, -0.1113,  0.0865, -0.0367, -0.0893, -0.0765, -0.1379,\n",
      "          -0.0951, -0.0028,  0.0997, -0.0506, -0.0107, -0.1355,  0.0400,\n",
      "           0.1333, -0.0210,  0.0189,  0.0169,  0.0744,  0.0454, -0.0676,\n",
      "          -0.1210, -0.0486,  0.0269,  0.0659, -0.1328, -0.0636, -0.1140,\n",
      "           0.0248, -0.0072,  0.0932, -0.0590, -0.0233, -0.1226,  0.0883,\n",
      "          -0.0500,  0.0264, -0.0554,  0.0326,  0.0934]],\n",
      "\n",
      "        [[ 0.1238, -0.1221,  0.0420, -0.0408,  0.0734,  0.0364,  0.0165,\n",
      "           0.0541,  0.0391, -0.0768, -0.0652, -0.1013, -0.0599, -0.0190,\n",
      "           0.0491, -0.1362, -0.0462, -0.0349,  0.0358, -0.1169, -0.0418,\n",
      "           0.1106,  0.0367,  0.0574,  0.1245, -0.0562,  0.0933,  0.0673,\n",
      "          -0.0715, -0.0657,  0.0906,  0.0146, -0.0973, -0.0226,  0.0438,\n",
      "          -0.1416, -0.1278, -0.1001,  0.0519,  0.1368, -0.0944, -0.1301,\n",
      "           0.1151, -0.0559,  0.0450, -0.1343, -0.0219, -0.0382,  0.1387,\n",
      "           0.0247,  0.0907, -0.0661, -0.1329,  0.0242,  0.1082,  0.1007,\n",
      "          -0.0115,  0.0537,  0.0439,  0.1334, -0.0432,  0.0061,  0.1171,\n",
      "          -0.0680,  0.1277,  0.1181, -0.0460,  0.0634, -0.0178,  0.0561,\n",
      "          -0.1169,  0.1064, -0.0148, -0.0384, -0.0318,  0.1126, -0.0193,\n",
      "           0.0851, -0.1072,  0.0472, -0.0466, -0.0720, -0.0914,  0.0522,\n",
      "          -0.0345,  0.0587,  0.0566,  0.1409,  0.1207, -0.1106, -0.0406,\n",
      "          -0.0816, -0.0962, -0.1223, -0.0644,  0.1297, -0.1083,  0.0930,\n",
      "           0.1150, -0.0735,  0.0028,  0.0964, -0.1102,  0.0187,  0.1286,\n",
      "           0.0921, -0.1113,  0.0865, -0.0367, -0.0893, -0.0765, -0.1379,\n",
      "          -0.0951, -0.0028,  0.0997, -0.0506, -0.0107, -0.1355,  0.0400,\n",
      "           0.1333, -0.0210,  0.0189,  0.0169,  0.0744,  0.0454, -0.0676,\n",
      "          -0.1210, -0.0486,  0.0269,  0.0659, -0.1328, -0.0636, -0.1140,\n",
      "           0.0248, -0.0072,  0.0932, -0.0590, -0.0233, -0.1226,  0.0883,\n",
      "          -0.0500,  0.0264, -0.0554,  0.0326,  0.0934]],\n",
      "\n",
      "        [[ 0.1238, -0.1221,  0.0420, -0.0408,  0.0734,  0.0364,  0.0165,\n",
      "           0.0541,  0.0391, -0.0768, -0.0652, -0.1013, -0.0599, -0.0190,\n",
      "           0.0491, -0.1362, -0.0462, -0.0349,  0.0358, -0.1169, -0.0418,\n",
      "           0.1106,  0.0367,  0.0574,  0.1245, -0.0562,  0.0933,  0.0673,\n",
      "          -0.0715, -0.0657,  0.0906,  0.0146, -0.0973, -0.0226,  0.0438,\n",
      "          -0.1416, -0.1278, -0.1001,  0.0519,  0.1368, -0.0944, -0.1301,\n",
      "           0.1151, -0.0559,  0.0450, -0.1343, -0.0219, -0.0382,  0.1387,\n",
      "           0.0247,  0.0907, -0.0661, -0.1329,  0.0242,  0.1082,  0.1007,\n",
      "          -0.0115,  0.0537,  0.0439,  0.1334, -0.0432,  0.0061,  0.1171,\n",
      "          -0.0680,  0.1277,  0.1181, -0.0460,  0.0634, -0.0178,  0.0561,\n",
      "          -0.1169,  0.1064, -0.0148, -0.0384, -0.0318,  0.1126, -0.0193,\n",
      "           0.0851, -0.1072,  0.0472, -0.0466, -0.0720, -0.0914,  0.0522,\n",
      "          -0.0345,  0.0587,  0.0566,  0.1409,  0.1207, -0.1106, -0.0406,\n",
      "          -0.0816, -0.0962, -0.1223, -0.0644,  0.1297, -0.1083,  0.0930,\n",
      "           0.1150, -0.0735,  0.0028,  0.0964, -0.1102,  0.0187,  0.1286,\n",
      "           0.0921, -0.1113,  0.0865, -0.0367, -0.0893, -0.0765, -0.1379,\n",
      "          -0.0951, -0.0028,  0.0997, -0.0506, -0.0107, -0.1355,  0.0400,\n",
      "           0.1333, -0.0210,  0.0189,  0.0169,  0.0744,  0.0454, -0.0676,\n",
      "          -0.1210, -0.0486,  0.0269,  0.0659, -0.1328, -0.0636, -0.1140,\n",
      "           0.0248, -0.0072,  0.0932, -0.0590, -0.0233, -0.1226,  0.0883,\n",
      "          -0.0500,  0.0264, -0.0554,  0.0326,  0.0934]],\n",
      "\n",
      "        [[ 0.1238, -0.1221,  0.0420, -0.0408,  0.0734,  0.0364,  0.0165,\n",
      "           0.0541,  0.0391, -0.0768, -0.0652, -0.1013, -0.0599, -0.0190,\n",
      "           0.0491, -0.1362, -0.0462, -0.0349,  0.0358, -0.1169, -0.0418,\n",
      "           0.1106,  0.0367,  0.0574,  0.1245, -0.0562,  0.0933,  0.0673,\n",
      "          -0.0715, -0.0657,  0.0906,  0.0146, -0.0973, -0.0226,  0.0438,\n",
      "          -0.1416, -0.1278, -0.1001,  0.0519,  0.1368, -0.0944, -0.1301,\n",
      "           0.1151, -0.0559,  0.0450, -0.1343, -0.0219, -0.0382,  0.1387,\n",
      "           0.0247,  0.0907, -0.0661, -0.1329,  0.0242,  0.1082,  0.1007,\n",
      "          -0.0115,  0.0537,  0.0439,  0.1334, -0.0432,  0.0061,  0.1171,\n",
      "          -0.0680,  0.1277,  0.1181, -0.0460,  0.0634, -0.0178,  0.0561,\n",
      "          -0.1169,  0.1064, -0.0148, -0.0384, -0.0318,  0.1126, -0.0193,\n",
      "           0.0851, -0.1072,  0.0472, -0.0466, -0.0720, -0.0914,  0.0522,\n",
      "          -0.0345,  0.0587,  0.0566,  0.1409,  0.1207, -0.1106, -0.0406,\n",
      "          -0.0816, -0.0962, -0.1223, -0.0644,  0.1297, -0.1083,  0.0930,\n",
      "           0.1150, -0.0735,  0.0028,  0.0964, -0.1102,  0.0187,  0.1286,\n",
      "           0.0921, -0.1113,  0.0865, -0.0367, -0.0893, -0.0765, -0.1379,\n",
      "          -0.0951, -0.0028,  0.0997, -0.0506, -0.0107, -0.1355,  0.0400,\n",
      "           0.1333, -0.0210,  0.0189,  0.0169,  0.0744,  0.0454, -0.0676,\n",
      "          -0.1210, -0.0486,  0.0269,  0.0659, -0.1328, -0.0636, -0.1140,\n",
      "           0.0248, -0.0072,  0.0932, -0.0590, -0.0233, -0.1226,  0.0883,\n",
      "          -0.0500,  0.0264, -0.0554,  0.0326,  0.0934]]],\n",
      "       grad_fn=<RepeatBackward0>)\n",
      "tensor([[1.4286e-01, 5.0661e-01, 5.4511e-01, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.9206e-01, 0.0000e+00, 7.3000e+01, 1.4286e-01, 9.0000e+00, 9.0909e-02,\n",
      "         2.0000e+00, 2.0635e-01, 0.0000e+00, 5.3403e-01, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 5.4511e-01, 1.0000e+00, 6.8493e-02, 6.4206e-01, 1.0000e+00,\n",
      "         5.7614e-01, 4.2857e-01, 4.4730e+04, 5.4511e-01, 1.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.4286e-01, 1.5000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 5.0000e-01, 5.1190e-01, 2.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.1667e-01, 6.3492e-02, 5.0000e-01, 1.0000e+00, 1.4286e-01,\n",
      "         0.0000e+00, 0.0000e+00, 2.0000e+00, 5.0000e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.3000e+01, 0.0000e+00, 1.4286e-01, 1.0000e+00,\n",
      "         5.4511e-01, 1.1111e-01, 1.1364e-01, 5.4511e-01, 1.0000e+00, 4.9206e-01,\n",
      "         2.2222e-01, 2.2222e-01, 5.4511e-01, 1.4286e-01, 5.3125e-01, 1.0000e+00,\n",
      "         0.0000e+00, 5.4392e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00, 2.0548e-01, 1.0000e+00, 0.0000e+00, 2.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0000e+00, 4.1667e-01, 1.0000e+00, 0.0000e+00,\n",
      "         4.0000e+00, 5.4392e-01, 0.0000e+00, 0.0000e+00, 1.4286e-01, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00, 5.0000e+00, 1.0000e+00, 1.4286e-01, 5.4511e-01,\n",
      "         1.0000e+00, 2.0000e+00, 0.0000e+00, 9.0000e+00, 0.0000e+00, 2.0000e+00,\n",
      "         5.0000e+00, 4.0000e-01, 0.0000e+00, 1.0000e+00, 2.0000e+00, 5.4511e-01,\n",
      "         1.0000e+00, 5.4885e+01, 5.0857e-01, 1.0000e+00, 0.0000e+00, 1.1765e-01,\n",
      "         4.4800e+04, 6.3000e+01, 3.8095e-01, 1.0000e+00, 2.0000e+00, 0.0000e+00,\n",
      "         2.5741e-01, 6.7571e-01, 5.0857e-01, 2.0000e-01, 1.4286e-01, 1.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 5.4511e-01, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00, 2.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         1.4286e-01],\n",
      "        [4.1703e-01, 3.5066e-01, 4.0346e-01, 2.0000e+00, 2.0000e+00, 0.0000e+00,\n",
      "         2.3341e-01, 0.0000e+00, 2.0900e+02, 7.1429e-02, 1.6000e+01, 5.0000e-01,\n",
      "         0.0000e+00, 4.1143e-01, 0.0000e+00, 4.3860e-01, 1.0000e+00, 2.0000e+00,\n",
      "         2.0000e+00, 3.6082e-01, 0.0000e+00, 1.8660e-01, 2.5079e-01, 1.0000e+00,\n",
      "         4.0388e-01, 7.1429e-02, 3.0000e+01, 4.5022e-01, 1.0000e+00, 2.0000e+00,\n",
      "         0.0000e+00, 2.0000e+00, 1.0000e+00, 1.0000e+00, 8.0000e-01, 8.6000e+01,\n",
      "         1.0000e+00, 2.5000e-01, 2.8571e-01, 2.2857e-01, 7.0000e+00, 1.0000e+00,\n",
      "         2.0000e+00, 7.1429e-02, 1.8286e-01, 2.8571e-01, 5.0000e-01, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 2.0000e+01, 2.8571e-01, 2.0000e+00, 0.0000e+00,\n",
      "         2.5000e-01, 0.0000e+00, 7.2000e+01, 0.0000e+00, 3.6364e-01, 4.0000e+00,\n",
      "         4.0346e-01, 4.3750e-01, 1.7544e-01, 4.5022e-01, 2.0000e+00, 2.3341e-01,\n",
      "         5.6250e-01, 5.0000e-01, 4.0346e-01, 3.4176e-01, 4.3772e-01, 4.0000e+00,\n",
      "         0.0000e+00, 6.6157e-01, 2.0000e+00, 2.0000e+00, 0.0000e+00, 3.0000e+00,\n",
      "         1.0000e+00, 2.0000e+00, 4.1148e-01, 3.0000e+00, 2.0000e+00, 7.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 9.0000e+00, 7.1429e-02, 1.0000e+00, 0.0000e+00,\n",
      "         7.0000e+00, 5.2511e-01, 1.0000e+00, 1.0000e+00, 3.6364e-01, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 9.7000e+01, 0.0000e+00, 4.1703e-01, 3.6082e-01,\n",
      "         0.0000e+00, 2.0000e+00, 1.0000e+00, 2.0000e+01, 2.0000e+00, 7.0000e+00,\n",
      "         9.0000e+00, 2.0619e-01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 3.6082e-01,\n",
      "         0.0000e+00, 5.7606e+01, 4.9481e-01, 0.0000e+00, 0.0000e+00, 2.3077e-01,\n",
      "         4.4700e+04, 1.7500e+02, 2.8571e-01, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.1474e-01, 3.4586e-01, 4.9481e-01, 6.1856e-02, 7.1429e-01, 1.0000e+00,\n",
      "         2.0000e+00, 0.0000e+00, 4.5022e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+00,\n",
      "         3.6364e-01],\n",
      "        [2.5595e-01, 6.2619e-01, 3.7607e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.1538e-01, 0.0000e+00, 5.8000e+01, 7.7778e-01, 1.0000e+00, 4.2857e-01,\n",
      "         2.0000e+00, 3.1915e-01, 0.0000e+00, 5.3428e-01, 0.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 3.7607e-01, 1.0000e+00, 2.0690e-01, 7.7778e-01, 0.0000e+00,\n",
      "         7.7778e-01, 4.2857e-01, 4.4635e+04, 3.7607e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2857e-01, 1.7000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 6.2619e-01, 2.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 2.3404e-01, 0.0000e+00, 5.0000e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.0000e-01, 0.0000e+00, 1.5000e+01, 0.0000e+00, 3.0769e-01, 2.0000e+00,\n",
      "         3.7607e-01, 0.0000e+00, 1.9608e-01, 3.7607e-01, 0.0000e+00, 8.1538e-01,\n",
      "         0.0000e+00, 2.0000e-01, 3.7607e-01, 2.5595e-01, 5.2794e-01, 1.0000e+00,\n",
      "         0.0000e+00, 3.5223e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00, 2.9310e-01, 0.0000e+00, 0.0000e+00, 2.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.0000e+00, 6.9412e-01, 0.0000e+00, 1.0000e+00, 3.0769e-01, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 1.8000e+01, 0.0000e+00, 2.5595e-01, 3.7607e-01,\n",
      "         0.0000e+00, 1.0000e+00, 0.0000e+00, 5.0000e+00, 1.0000e+00, 2.0000e+00,\n",
      "         8.0000e+00, 1.1111e-01, 3.0769e-01, 0.0000e+00, 1.0000e+00, 3.7607e-01,\n",
      "         0.0000e+00, 6.2596e+01, 5.8745e-01, 0.0000e+00, 0.0000e+00, 2.0000e-01,\n",
      "         5.0420e+04, 4.7000e+01, 6.6270e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.8785e-01, 7.7778e-01, 5.8745e-01, 5.5556e-02, 3.0769e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.7607e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0769e-01],\n",
      "        [0.0000e+00, 6.7857e-01, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.0412e-01, 0.0000e+00, 1.2600e+02, 2.1429e-01, 9.0000e+00, 1.2500e-01,\n",
      "         2.0000e+00, 3.4259e-01, 0.0000e+00, 4.8727e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 6.3492e-02, 4.2294e-01, 0.0000e+00,\n",
      "         4.7381e-01, 2.1429e-01, 3.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1000e+01,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 6.2857e-01, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 2.1429e-01, 7.4074e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.7000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.1111e-01, 2.9682e-02, 0.0000e+00, 0.0000e+00, 5.0412e-01,\n",
      "         2.2222e-01, 1.2500e-01, 0.0000e+00, 0.0000e+00, 4.8840e-01, 1.0000e+00,\n",
      "         0.0000e+00, 5.2500e-01, 2.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.2540e-01, 2.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0000e+00, 2.1429e-01, 1.0000e+00, 0.0000e+00,\n",
      "         4.0000e+00, 5.2500e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 5.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 2.0000e+00, 0.0000e+00, 8.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.0000e+00, 4.0000e-01, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 5.5792e+01, 5.5104e-01, 1.0000e+00, 0.0000e+00, 4.7619e-02,\n",
      "         0.0000e+00, 1.0800e+02, 4.0476e-01, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         5.6250e-01, 4.5766e-01, 5.5104e-01, 2.0000e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [5.3929e-01, 4.4844e-01, 4.1071e-01, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.8207e-01, 0.0000e+00, 1.3300e+02, 7.1429e-02, 1.0000e+01, 2.5000e-01,\n",
      "         1.0000e+00, 3.0973e-01, 0.0000e+00, 4.8478e-01, 1.0000e+00, 2.0000e+00,\n",
      "         2.0000e+00, 4.1071e-01, 0.0000e+00, 1.1278e-01, 2.1868e-01, 0.0000e+00,\n",
      "         2.8858e-01, 7.1429e-02, 4.4605e+04, 4.1071e-01, 1.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 8.7500e-01, 4.3000e+01,\n",
      "         0.0000e+00, 2.3077e-01, 5.0000e-01, 4.0549e-01, 2.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 4.1667e-01, 9.7345e-02, 5.0000e-01, 3.5000e-01, 2.5000e-01,\n",
      "         0.0000e+00, 0.0000e+00, 2.1000e+01, 5.0000e-01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e-01, 0.0000e+00, 3.5000e+01, 0.0000e+00, 2.5000e-01, 2.0000e+01,\n",
      "         4.1071e-01, 1.0000e-01, 2.7778e-02, 4.1071e-01, 1.0000e+00, 5.8207e-01,\n",
      "         7.0000e-01, 3.0769e-01, 4.1071e-01, 3.9881e-01, 4.9703e-01, 2.0000e+00,\n",
      "         0.0000e+00, 5.5157e-01, 0.0000e+00, 2.0000e+00, 1.0000e+00, 2.0000e+00,\n",
      "         0.0000e+00, 7.0000e+00, 3.2331e-01, 4.0000e+00, 1.0000e+00, 2.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 7.0000e+00, 4.1667e-01, 1.0000e+00, 0.0000e+00,\n",
      "         3.0000e+00, 5.2230e-01, 1.0000e+00, 1.0000e+00, 2.5000e-01, 0.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 9.8000e+01, 0.0000e+00, 4.6225e-01, 4.1071e-01,\n",
      "         0.0000e+00, 1.0000e+00, 1.0000e+00, 1.3000e+01, 0.0000e+00, 2.0000e+00,\n",
      "         8.0000e+00, 2.1429e-01, 0.0000e+00, 1.0000e+00, 4.0000e+00, 4.1071e-01,\n",
      "         0.0000e+00, 5.9874e+01, 4.8258e-01, 1.0000e+00, 0.0000e+00, 3.4483e-02,\n",
      "         5.4855e+04, 1.1300e+02, 5.2381e-01, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         5.0641e-01, 2.2645e-01, 4.8258e-01, 6.1224e-02, 0.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 4.1071e-01, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.5000e-01]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pma(graph_X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[24], line 63\u001b[0m, in \u001b[0;36mPMA.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmab(a, X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[24], line 22\u001b[0m, in \u001b[0;36mMAB.forward\u001b[0;34m(self, Q, K)\u001b[0m\n\u001b[1;32m     20\u001b[0m dim_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_V \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads\n\u001b[1;32m     21\u001b[0m Q_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(Q\u001b[38;5;241m.\u001b[39msplit(dim_split, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m K_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(K\u001b[38;5;241m.\u001b[39msplit(dim_split, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m V_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(V\u001b[38;5;241m.\u001b[39msplit(dim_split, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(Q_\u001b[38;5;241m.\u001b[39mbmm(K_\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m/\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_V), \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:981\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(split_size, (\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mSymInt)):\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_VF\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m, split_size, dim)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_VF\u001b[38;5;241m.\u001b[39msplit_with_sizes(\u001b[38;5;28mself\u001b[39m, split_size, dim)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "pma(graph_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
